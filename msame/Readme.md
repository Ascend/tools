[中文](https://github.com/Ascend/tools/blob/master/msame/Readme_cn.md)|EN

# Ascend Model Executor: msame

### Overview

The tool runs an offline model (.om) on the binary input data and returns the model inference result. The tool allows you to run repeated inferences on the same input data.

The offline model fed into this tool must be generated by using Ascend Tensor Compiler (ATC). The binary input data must meet the model's input requirements. Inference on multiple inputs is supported.

### Environment Setup

Set up an operating environment powered by Ascend AI Processor. The development environment is optional if you do not need to rebuild the tool script.

### Tool Preparation

- In command line:

  **git clone https://github.com/Ascend/tools.git**

- By package download:

  1. Click **Clone or download** in the upper right corner of the tools repository and select **Download ZIP**.

  2. Upload the ZIP package to the home directory of a common user in the development environment, for example, **$HOME/ascend-tools-master.zip**.

  3. In the development environment, run the following command to unzip the package:

     **unzip ascend-tools-master.zip**

### Tool Instructions

#### a. Direct execution

**If an ARM-based operating environment is set up, you can run this tool directly. Otherwise, follow method b to build the source code first.**

Go to the directory:

```
cd $HOME/tools/msame/
```

Go to the **out** directory where the tool is located:

```
cd out
```

#### b. Build before execution

**Set up the development environment and operating environment in advance, either on the same server or on separate servers.** 
You can tweak the source code as needed in this step.

Set environment variables. The following is an example only. Replace **/home/HwHiAiUser/Ascend/ascend-toolkit/latest** with the actual ACLlib installation path.

**export DDK\_PATH=/home/HwHiAiUser/Ascend/ascend-toolkit/latest**
**export NPU\_HOST\_LIB=/home/HwHiAiUser/Ascend/ascend-toolkit/latest/acllib/lib64/stub**

Go to the **msame** directory:

```
cd $HOME/AscendProjects/tools/msame/
```

Run the build script:

```
./build.sh g++ $HOME/AscendProjects/tools/msame/out
```

The first argument specifies the compiler, which is determined by the operating environment. 
The second argument specifies the output directory. To specify a relative directory, ensure that it is relative to the **out** directory.

## Usage Examples

Example 1: without the **input** option

Fake data (all 0s) is constructed and fed to the model for inference.

```
./msame --model /home/HwHiAiUser/msame/colorization.om  --output /home/HwHiAiUser/msame/out/ --outfmt TXT --loop 1
```

Example 2: model inference on a single input

a. Specify a binary input file.

```
./msame --model /home/HwHiAiUser/msame/colorization.om --input /home/HwHiAiUser/msame/data/colorization_input.bin --output /home/HwHiAiUser/msame/out/ --outfmt TXT --loop 1
```

b. Specify an input directory containing binary files. In this case, the **--loop** option is invalid.

```
./msame --model /home/HwHiAiUser/msame/colorization.om --input /home/HwHiAiUser/msame/data --output /home/HwHiAiUser/msame/out/ --outfmt TXT
```

Example 3: model inference on multiple inputs

a. Specify multiple binary input files separated by commas (,). No space is allowed before or after the commas.

```
./msame --model /home/HwHiAiUser/msame/colorization.om --input /home/HwHiAiUser/msame/data1/a.bin,/home/HwHiAiUser/ljj/data2/a.bin --output /home/HwHiAiUser/msame/out/ --outfmt TXT  --loop 1
```

b. Specify multiple input directories separated by commas (,). No space is allowed before or after the commas. The binary files in the directories must have consistent file names. In this case, the **--loop** option is invalid.

```
./msame --model /home/HwHiAiUser/msame/colorization.om --input /home/HwHiAiUser/msame/data1,/home/HwHiAiUser/msame/data2 --output /home/HwHiAiUser/msame/out/ --outfmt TXT
```

For details about all available options, run the **--help** command.

## Restrictions

Make sure that the tool running user has enough permissions to create directories and run the tool. 
Dynamic batched inference is not supported.

## Command Line Options

| Option| Description
|----------|----------
| --model| Offline model.
| --input| Model input, either binary files or directories. If this option is not included, all-0s data is generated as the model input.
| --output| Inference output directory.
| --outfmt| Inference output format, either TXT or BIN.
| --loop| (Optional) Number of inferences. Must be in the range of \[1, 100]. Defaults to 1. When profiler is set to true, you are advised to set this option to 1.
| --debug| (Optional) Debug switch for printing the model description, either true or false. Defaults to false.
| --profiler| (Optional) Profiler switch, either true or false. Defaults to false.<br> The profiler data is stored in the profiler folder under the directory specified by the --output argument. This option and --dump must not be set to true at the same time.
| --dump| (Optional) Dump switch, either true or false. Defaults to false.<br> The dump data is stored in the dump folder under the directory specified by the --output argument. This option and --profiler must not be set to true at the same time.
| --device| (Optional) Inference device. Must be in the range of \[0, 255]. Defaults to 0.
| --help| Help information.