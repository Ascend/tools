diff -Nur origin/eval.py code/eval.py
--- origin/eval.py	2021-12-10 14:54:43.810000000 +0800
+++ code/eval.py	2021-12-10 14:54:43.820000000 +0800
@@ -84,6 +84,15 @@
     # eval model
     res = model.eval(dataset)
     print("result:", res, "ckpt=", config.checkpoint_file_path)
+    ACC_DIR = os.getenv("RESULT_PATH")
+    if ACC_DIR is None:
+        print("Warning: The environment variable 'RESULT_PATH' is not set. ")
+    elif not os.path.isdir(ACC_DIR):
+        print("Warning: The environment variable 'RESULT_PATH' is not a valid directory. ")
+    else:
+      ACC_LOG = os.path.join(ACC_DIR, "eval_acc.log")
+      with open(ACC_LOG, 'w') as f:
+          f.write("{}".format(res['top_1_accuracy']))
 
 if __name__ == '__main__':
     eval_net()
diff -Nur origin/train.py code/train.py
--- origin/train.py	2021-12-10 14:54:43.820000000 +0800
+++ code/train.py	2021-12-10 14:54:43.820000000 +0800
@@ -17,6 +17,7 @@
 import glob
 import os
 import numpy as np
+import time
 
 from mindspore import context
 from mindspore import Tensor
@@ -43,7 +44,12 @@
 from src.model_utils.moxing_adapter import moxing_wrapper
 from src.model_utils.device_adapter import get_rank_id, get_device_num
 from src.resnet import conv_variance_scaling_initializer
-
+try:
+   import ais_utils
+except ImportError:
+   ais_utils_is_existed = False
+else:
+   ais_utils_is_existed = True
 
 set_seed(1)
 
@@ -358,6 +364,7 @@
         config.run_eval = False
         logger.warning("Thor optimizer not support evaluation while training.")
 
+    model.build(dataset, None, sink_size=dataset.get_dataset_size(), epoch=config.epoch_size - config.pretrain_epoch_size)
     # define callbacks
     time_cb = TimeMonitor(data_size=step_size)
     loss_cb = LossCallBack(config.has_trained_epoch)
@@ -376,9 +383,34 @@
         config.epoch_size = config.train_epoch_size
     dataset_sink_mode = (not config.parameter_server) and target != "CPU"
     config.pretrain_epoch_size = config.has_trained_epoch
+
+    if ais_utils_is_existed:
+        start_time = ais_utils.get_datatime()
+    else:
+        start_time = time.time()
+
     model.train(config.epoch_size - config.pretrain_epoch_size, dataset, callbacks=cb,
                 sink_size=dataset.get_dataset_size(), dataset_sink_mode=dataset_sink_mode)
 
+    all_data_sum = step_size * config.batch_size * config.epoch_size
+    if ais_utils_is_existed:
+        end_time = ais_utils.get_datatime()
+        throughput_rate = ais_utils.calc_throughput_rate(all_data_sum, start_time, end_time)
+    else:
+        end_time = time.time()
+        throughput_rate = all_data_sum/(end_time - start_time)
+
+    rank_id = int(os.getenv('RANK_ID'))
+    THROUGHPUT_DIR = os.getenv("RESULT_PATH")
+    if THROUGHPUT_DIR is None:
+        print("Warning: The environment variable 'RESULT_PATH' is not set. ")
+    elif not os.path.isdir(THROUGHPUT_DIR):
+        print("Warning: The environment variable 'RESULT_PATH' is not a valid directory. ")
+    else:
+      THROUGHPUT_LOG = os.path.join(THROUGHPUT_DIR, "throughput_rank_{}".format(rank_id))
+      with open(THROUGHPUT_LOG, 'w') as f:
+          f.write("{}".format(throughput_rate))
+
     if config.run_eval and config.enable_cache:
         print("Remember to shut down the cache server via \"cache_admin --stop\"")
 
